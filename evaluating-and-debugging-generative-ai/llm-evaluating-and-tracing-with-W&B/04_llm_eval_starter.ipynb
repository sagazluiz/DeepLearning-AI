{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://learn.deeplearning.ai/evaluating-debugging-generative-ai/lesson/5/llm-evaluation-and-tracing-with-w&b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Evaluation and Tracing with W&B\n",
    "\n",
    "<!--- @wandbcode{dlai_04} -->\n",
    "\n",
    "## 1. Using Tables for Evaluation\n",
    "\n",
    "In this section, we will call OpenAI LLM to generate names of our game assets. We will use W&B Tables to evaluate the generations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T13:43:09.804536Z",
     "start_time": "2023-12-02T13:43:07.786338Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import openai\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential, # for exponential backoff\n",
    ")  \n",
    "import wandb\n",
    "from wandb.sdk.data_types.trace_tree import Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T13:43:42.322668Z",
     "start_time": "2023-12-02T13:43:42.301208Z"
    }
   },
   "outputs": [],
   "source": [
    "# get openai API key\n",
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T13:43:48.331463Z",
     "start_time": "2023-12-02T13:43:48.328120Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT = \"dlai_llm\"\n",
    "MODEL_NAME = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T13:43:54.953602Z",
     "start_time": "2023-12-02T13:43:51.477952Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfabio-bd-araujo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T13:44:32.402267Z",
     "start_time": "2023-12-02T13:44:12.116751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/r337555/Projects-Personal/DeepLearning-AI/evaluating-and-debugging-generative-ai/llm-evaluating-and-tracing-with-W&B/wandb/run-20231202_104412-jsiajde5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fabio-bd-araujo/dlai_llm/runs/jsiajde5' target=\"_blank\">radiant-lion-1</a></strong> to <a href='https://wandb.ai/fabio-bd-araujo/dlai_llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fabio-bd-araujo/dlai_llm' target=\"_blank\">https://wandb.ai/fabio-bd-araujo/dlai_llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fabio-bd-araujo/dlai_llm/runs/jsiajde5' target=\"_blank\">https://wandb.ai/fabio-bd-araujo/dlai_llm/runs/jsiajde5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=PROJECT, job_type=\"generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple generations\n",
    "Let's start by generating names for our game assets using OpenAI `ChatCompletion`, and saving the resulting generations in W&B Tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T13:45:19.580603Z",
     "start_time": "2023-12-02T13:45:19.574472Z"
    }
   },
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.ChatCompletion.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T13:55:57.073343Z",
     "start_time": "2023-12-02T13:55:56.995040Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return client.chat.completions.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T13:53:35.809199Z",
     "start_time": "2023-12-02T13:53:35.799856Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_and_print(system_prompt, user_prompt, table, n=5):\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    start_time = time.time()\n",
    "    responses = completion_with_backoff(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        n = n,\n",
    "        )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    for response in responses.choices:\n",
    "        generation = response.message.content\n",
    "        print(generation)\n",
    "    table.add_data(system_prompt,\n",
    "                user_prompt,\n",
    "                [response.message.content for response in responses.choices],\n",
    "                elapsed_time,\n",
    "                datetime.datetime.fromtimestamp(responses.created),\n",
    "                responses.model,\n",
    "                responses.usage.prompt_tokens,\n",
    "                responses.usage.completion_tokens,\n",
    "                responses.usage.total_tokens\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:06:50.394193Z",
     "start_time": "2023-12-02T14:06:50.384121Z"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio  # Importe a biblioteca asyncio\n",
    "\n",
    "async def generate_and_print(system_prompt, user_prompt, table, n=5):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use 'await' para aguardar a execução da coroutine\n",
    "    responses = await completion_with_backoff(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        n=n,\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    for response in responses.choices:\n",
    "        generation = response.message.content\n",
    "        print(generation)\n",
    "\n",
    "    table.add_data(system_prompt,\n",
    "                   user_prompt,\n",
    "                   [response.message.content for response in responses.choices],\n",
    "                   elapsed_time,\n",
    "                   datetime.datetime.fromtimestamp(responses.created),\n",
    "                   responses.model,\n",
    "                   responses.usage.prompt_tokens,\n",
    "                   responses.usage.completion_tokens,\n",
    "                   responses.usage.total_tokens\n",
    "                   )\n",
    "\n",
    "# Para chamar a função, use asyncio.run se estiver fora de outro contexto async\n",
    "# Exemplo: asyncio.run(generate_and_print(system_prompt, user_prompt, table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T13:53:39.079020Z",
     "start_time": "2023-12-02T13:53:39.073473Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a creative copywriter.\n",
    "You're given a category of game asset, \\\n",
    "and your goal is to design a name of that asset.\n",
    "The game is set in a fantasy world \\\n",
    "where everyone laughs and respects each other, \n",
    "while celebrating diversity.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T13:53:39.844143Z",
     "start_time": "2023-12-02T13:53:39.838183Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define W&B Table to store generations\n",
    "columns = [\"system_prompt\", \"user_prompt\", \"generations\", \"elapsed_time\", \"timestamp\",\\\n",
    "            \"model\", \"prompt_tokens\", \"completion_tokens\", \"total_tokens\"]\n",
    "table = wandb.Table(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:06:46.739352Z",
     "start_time": "2023-12-02T14:06:46.684371Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'coroutine' object has no attribute 'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m user_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhero\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgenerate_and_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m, in \u001b[0;36mgenerate_and_print\u001b[0;34m(system_prompt, user_prompt, table, n)\u001b[0m\n\u001b[1;32m      7\u001b[0m responses \u001b[38;5;241m=\u001b[39m completion_with_backoff(\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39mMODEL_NAME,\n\u001b[1;32m      9\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m     10\u001b[0m     n \u001b[38;5;241m=\u001b[39m n,\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresponses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m:\n\u001b[1;32m     14\u001b[0m     generation \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(generation)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'coroutine' object has no attribute 'choices'"
     ]
    }
   ],
   "source": [
    "user_prompt = \"hero\"\n",
    "generate_and_print(system_prompt, user_prompt, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:07:50.703190Z",
     "start_time": "2023-12-02T14:07:47.468150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity's Chorus\n",
      "Harmonic Champions\n",
      "Harmony Chosen\n",
      "Unity's Pride\n",
      "Title: \"Mirthful Vanguard\"\n",
      "\n",
      "Description: In this fantastical realm, heroes embody the values of laughter, respect, and unity. The Mirthful Vanguard represents a formidable force, combining strength and compassion to protect the realm from darkness. These heroes inspire others through their diverse backgrounds, fostering a harmonious society that celebrates each individual's unique strengths with a lighthearted touch. Join the Mirthful Vanguard and let your laughter guide you to victory!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_prompt = \"hero\"\n",
    "# Use asyncio.run para executar a função assíncrona\n",
    "\n",
    "await generate_and_print(system_prompt, user_prompt, table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"jewel\"\n",
    "generate_and_print(system_prompt, user_prompt, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:13:33.087609Z",
     "start_time": "2023-12-02T14:13:30.411318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Harmony Gems\n",
      "\n",
      "Explanation: In this fantasy world, jewels hold a special significance, symbolizing diversity and unity. The name \"Harmony Gems\" reflects the idea of different gems coming together to create a harmonious and beautiful collection. Each gem represents a unique element or quality, and when combined, they create a powerful and dazzling force for good. The name also emphasizes the values of laughter, respect, and celebration of diversity that are integral to the game's world.\n",
      "Sparkle Oasis\n",
      "Harmony Gems\n",
      "Gleamstone\n",
      "Harmony Gems\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"jewel\"\n",
    "# Use asyncio.run para executar a função assíncrona\n",
    "\n",
    "await generate_and_print(system_prompt, user_prompt, table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:14:17.910973Z",
     "start_time": "2023-12-02T14:14:04.634924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">radiant-lion-1</strong> at: <a href='https://wandb.ai/fabio-bd-araujo/dlai_llm/runs/jsiajde5' target=\"_blank\">https://wandb.ai/fabio-bd-araujo/dlai_llm/runs/jsiajde5</a><br/> View job at <a href='https://wandb.ai/fabio-bd-araujo/dlai_llm/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMDMzMDU0Ng==/version_details/v0' target=\"_blank\">https://wandb.ai/fabio-bd-araujo/dlai_llm/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMDMzMDU0Ng==/version_details/v0</a><br/>Synced 6 W&B file(s), 1 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231202_104412-jsiajde5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({\"simple_generations\": table})\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Tracer to log more complex chains\n",
    "\n",
    "How can we get more creative outputs? Let's design an LLM chain that will first randomly pick a fantasy world, and then generate character names. We will demonstrate how to use Tracer in such scenario. We will log the inputs and outputs, start and end times, whether the OpenAI call was successful, the token usage, and additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:27:53.152552Z",
     "start_time": "2023-12-02T14:27:53.145608Z"
    }
   },
   "outputs": [],
   "source": [
    "worlds = [\n",
    "    \"a mystic medieval island inhabited by intelligent and funny frogs\",\n",
    "    \"a modern castle sitting on top of a volcano in a faraway galaxy\",\n",
    "    \"a digital world inhabited by friendly machine learning engineers\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:27:57.691063Z",
     "start_time": "2023-12-02T14:27:57.685403Z"
    }
   },
   "outputs": [],
   "source": [
    "# define your config\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "temperature = 0.7\n",
    "system_message = \"\"\"You are a creative copywriter. \n",
    "You're given a category of game asset and a fantasy world.\n",
    "Your goal is to design a name of that asset.\n",
    "Provide the resulting name only, no additional description.\n",
    "Single name, max 3 words output, remember!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:42:50.333033Z",
     "start_time": "2023-12-02T14:42:50.317566Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_creative_chain(query):\n",
    "    # part 1 - a chain is started...\n",
    "    start_time_ms = round(datetime.datetime.now().timestamp() * 1000)\n",
    "\n",
    "    root_span = Trace(\n",
    "          name=\"MyCreativeChain\",\n",
    "          kind=\"chain\",\n",
    "          start_time_ms=start_time_ms,\n",
    "          metadata={\"user\": \"student_1\"},\n",
    "          model_dict={\"_kind\": \"CreativeChain\"}\n",
    "          )\n",
    "\n",
    "    # part 2 - your chain picks a fantasy world\n",
    "    time.sleep(3)\n",
    "    world = random.choice(worlds)\n",
    "    expanded_prompt = f'Game asset category: {query}; fantasy world description: {world}'\n",
    "    tool_end_time_ms = round(datetime.datetime.now().timestamp() * 1000)\n",
    "\n",
    "    # create a Tool span \n",
    "    tool_span = Trace(\n",
    "          name=\"WorldPicker\",\n",
    "          kind=\"tool\",\n",
    "          status_code=\"success\",\n",
    "          start_time_ms=start_time_ms,\n",
    "          end_time_ms=tool_end_time_ms,\n",
    "          inputs={\"input\": query},\n",
    "          outputs={\"result\": expanded_prompt},\n",
    "          model_dict={\"_kind\": \"tool\", \"num_worlds\": len(worlds)}\n",
    "          )\n",
    "\n",
    "    # add the TOOL span as a child of the root\n",
    "    root_span.add_child(tool_span)\n",
    "\n",
    "    # part 3 - the LLMChain calls an OpenAI LLM...\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": system_message},\n",
    "      {\"role\": \"user\", \"content\": expanded_prompt}\n",
    "    ]\n",
    "\n",
    "    response = completion_with_backoff(model=model_name,\n",
    "                                       messages=messages,\n",
    "                                       max_tokens=12,\n",
    "                                       temperature=temperature)   \n",
    "\n",
    "    llm_end_time_ms = round(datetime.datetime.now().timestamp() * 1000)\n",
    "    response_text = response[0][\"message\"][\"content\"]\n",
    "    token_usage = response[\"usage\"].to_dict()\n",
    "\n",
    "    llm_span = Trace(\n",
    "          name=\"OpenAI\",\n",
    "          kind=\"llm\",\n",
    "          status_code=\"success\",\n",
    "          metadata={\"temperature\":temperature,\n",
    "                    \"token_usage\": token_usage, \n",
    "                    \"model_name\":model_name},\n",
    "          start_time_ms=tool_end_time_ms,\n",
    "          end_time_ms=llm_end_time_ms,\n",
    "          inputs={\"system_prompt\":system_message, \"query\":expanded_prompt},\n",
    "          outputs={\"response\": response_text},\n",
    "          model_dict={\"_kind\": \"Openai\", \"engine\": response[\"model\"], \"model\": response[\"object\"]}\n",
    "          )\n",
    "\n",
    "    # add the LLM span as a child of the Chain span...\n",
    "    root_span.add_child(llm_span)\n",
    "\n",
    "    # update the end time of the Chain span\n",
    "    root_span.add_inputs_and_outputs(\n",
    "          inputs={\"query\":query},\n",
    "          outputs={\"response\": response_text})\n",
    "\n",
    "    # update the Chain span's end time\n",
    "    root_span.end_time_ms = llm_end_time_ms\n",
    "\n",
    "\n",
    "    # part 4 - log all spans to W&B by logging the root span\n",
    "    root_span.log(name=\"creative_trace\")\n",
    "    print(f\"Result: {response_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:33:07.421960Z",
     "start_time": "2023-12-02T14:32:48.220491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/r337555/Projects-Personal/DeepLearning-AI/evaluating-and-debugging-generative-ai/llm-evaluating-and-tracing-with-W&B/wandb/run-20231202_113248-g6yzt7ih</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fabio-bd-araujo/dlai_llm/runs/g6yzt7ih' target=\"_blank\">pious-flower-2</a></strong> to <a href='https://wandb.ai/fabio-bd-araujo/dlai_llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fabio-bd-araujo/dlai_llm' target=\"_blank\">https://wandb.ai/fabio-bd-araujo/dlai_llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fabio-bd-araujo/dlai_llm/runs/g6yzt7ih' target=\"_blank\">https://wandb.ai/fabio-bd-araujo/dlai_llm/runs/g6yzt7ih</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fabio-bd-araujo/dlai_llm/runs/g6yzt7ih?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f99c9d0f2b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start a new wandb run\n",
    "wandb.init(project=PROJECT, job_type=\"generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:42:57.446888Z",
     "start_time": "2023-12-02T14:42:54.368147Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'coroutine' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_creative_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhero\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 46\u001b[0m, in \u001b[0;36mrun_creative_chain\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     40\u001b[0m response \u001b[38;5;241m=\u001b[39m completion_with_backoff(model\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m     41\u001b[0m                                    messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m     42\u001b[0m                                    max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[1;32m     43\u001b[0m                                    temperature\u001b[38;5;241m=\u001b[39mtemperature)   \n\u001b[1;32m     45\u001b[0m llm_end_time_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mtimestamp() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m response_text \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     47\u001b[0m token_usage \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m     49\u001b[0m llm_span \u001b[38;5;241m=\u001b[39m Trace(\n\u001b[1;32m     50\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     51\u001b[0m       kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m       model_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenai\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m     61\u001b[0m       )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'coroutine' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "run_creative_chain(\"hero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_creative_chain(\"jewel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain agent\n",
    "\n",
    "In the third scenario, we'll introduce an agent that will use tools such as WorldPicker and NameValidator to come up with the ultimate name. We will also use Langchain here and demonstrate its W&B integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import things that are needed generically\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=PROJECT, job_type=\"generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldPickerTool(BaseTool):\n",
    "    name = \"pick_world\"\n",
    "    description = \"pick a virtual game world for your character or item naming\"\n",
    "    worlds = [\n",
    "                \"a mystic medieval island inhabited by intelligent and funny frogs\",\n",
    "                \"a modern anthill featuring a cyber-ant queen and her cyber-ant-workers\",\n",
    "                \"a digital world inhabited by friendly machine learning engineers\"\n",
    "            ]\n",
    "\n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        time.sleep(1)\n",
    "        return random.choice(self.worlds)\n",
    "\n",
    "    async def _arun(\n",
    "        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"pick_world does not support async\")\n",
    "        \n",
    "class NameValidatorTool(BaseTool):\n",
    "    name = \"validate_name\"\n",
    "    description = \"validate if the name is properly generated\"\n",
    "\n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        time.sleep(1)\n",
    "        if len(query) < 20:\n",
    "            return f\"This is a correct name: {query}\"\n",
    "        else:\n",
    "            return f\"This name is too long. It should be shorter than 20 characters.\"\n",
    "\n",
    "    async def _arun(\n",
    "        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"validate_name does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tools = [WorldPickerTool(), NameValidatorTool()]\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\n",
    "    \"Find a virtual game world for me and imagine the name of a hero in that world\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\n",
    "    \"Find a virtual game world for me and imagine the name of a jewel in that world\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\n",
    "    \"Find a virtual game world for me and imagine the name of food in that world.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: LLM outputs are variable. Your results may not match those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
