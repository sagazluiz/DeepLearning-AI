{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a language model\n",
    "\n",
    "<!--- @wandbcode{dlai_05} -->\n",
    "\n",
    "Let's see how to finetune a language model to generate character backstories using HuggingFace Trainer with wandb integration. We'll use a tiny language model (`TinyStories-33M`) due to resource constraints, but the lessons you learn here should be applicable to large models too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:00:45.646901Z",
     "start_time": "2023-12-14T02:00:34.829187Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import transformers\n",
    "transformers.set_seed(42)\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:00:49.852459Z",
     "start_time": "2023-12-14T02:00:47.242382Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfabio-bd-araujo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:00:53.234621Z",
     "start_time": "2023-12-14T02:00:53.231248Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"roneneldan/TinyStories-33M\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data\n",
    "\n",
    "We'll start by loading a dataset containing Dungeons and Dragons character biographies from Huggingface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can expect to get some warning here, this is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:01:02.006369Z",
     "start_time": "2023-12-14T02:00:58.564721Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = load_dataset('MohamedRashad/characters_backstories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:01:04.074462Z",
     "start_time": "2023-12-14T02:01:04.068466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Generate Backstory based on following information\\nCharacter Name: Dewin \\nCharacter Race: Halfling\\nCharacter Class: Sorcerer bard\\n\\nOutput:\\n',\n",
       " 'target': 'Dewin thought he was a wizard, but it turned out it was the draconic blood in his veins that brought him eldritch power.  Music classes in wizarding college taught him yet another use for his power, and when he was expelled he took up adventuring'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at one example\n",
    "ds[\"train\"][400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:01:05.695244Z",
     "start_time": "2023-12-14T02:01:05.688414Z"
    }
   },
   "outputs": [],
   "source": [
    "# As this dataset has no validation split, we will create one\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:01:15.865110Z",
     "start_time": "2023-12-14T02:01:11.913463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf22546f4bb44699294a8b29c42b471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/465 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We'll create a tokenizer from model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "\n",
    "# We'll need padding to have same length sequences in a batch\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define a tokenization function that first concatenates text and target\n",
    "def tokenize_function(example):\n",
    "    merged = example[\"text\"] + \" \" + example[\"target\"]\n",
    "    batch = tokenizer(merged, padding='max_length', truncation=True, max_length=128)\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
    "    return batch\n",
    "\n",
    "# Apply it on our dataset, and remove the text columns\n",
    "tokenized_datasets = ds.map(tokenize_function, remove_columns=[\"text\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:02:03.206622Z",
     "start_time": "2023-12-14T02:02:03.200775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Backstory based on following information\n",
      "Character Name: Mr. Gale\n",
      "Character Race: Half-orc\n",
      "Character Class: Cleric\n",
      "\n",
      "Output:\n",
      " Growing up the only half-orc in a small rural town was rough. His mother didn't survive childbirth and so was raised in a church in a high mountain pass, his attention was always drawn by airships passing through, and dreams of an escape. Leaving to strike out on his own as early as he could he made a living for most of his life as an airship sailor, and occasionally a pirate. A single storm visits him throughout his life, marking every major\n"
     ]
    }
   ],
   "source": [
    "# Let's check out one prepared example\n",
    "print(tokenizer.decode(tokenized_datasets[\"train\"][900]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Let's finetune a pretrained language model on our dataset using HF Transformers and their wandb integration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:02:09.217342Z",
     "start_time": "2023-12-14T02:02:07.270949Z"
    }
   },
   "outputs": [],
   "source": [
    "# We will train a causal (autoregressive) language model from a pretrained checkpoint\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:02:21.816121Z",
     "start_time": "2023-12-14T02:02:10.541086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/r337555/Projects-Personal/DeepLearning-AI/evaluating-and-debugging-generative-ai/finetuning-a-language-models/wandb/run-20231213_230210-3sfjxpr8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fabio-bd-araujo/dlai_lm_tuning/runs/3sfjxpr8' target=\"_blank\">devout-lion-2</a></strong> to <a href='https://wandb.ai/fabio-bd-araujo/dlai_lm_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fabio-bd-araujo/dlai_lm_tuning' target=\"_blank\">https://wandb.ai/fabio-bd-araujo/dlai_lm_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fabio-bd-araujo/dlai_lm_tuning/runs/3sfjxpr8' target=\"_blank\">https://wandb.ai/fabio-bd-araujo/dlai_lm_tuning/runs/3sfjxpr8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start a new wandb run\n",
    "run = wandb.init(project='dlai_lm_tuning', job_type=\"training\", anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:02:23.991675Z",
     "start_time": "2023-12-14T02:02:23.980002Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/r337555/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1252: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-characters-backstories\",\n",
    "    report_to=\"wandb\", # we need one line to track experiments in wandb\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=True, # force cpu use, will be renamed `use_cpu`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:02:28.327728Z",
     "start_time": "2023-12-14T02:02:28.302773Z"
    }
   },
   "outputs": [],
   "source": [
    "# We'll use HF Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:32:16.179456Z",
     "start_time": "2023-12-14T02:02:32.550047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='233' max='233' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [233/233 29:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.597100</td>\n",
       "      <td>3.353575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=233, training_loss=3.721863612596569, metrics={'train_runtime': 1783.401, 'train_samples_per_second': 1.041, 'train_steps_per_second': 0.131, 'total_flos': 40423258718208.0, 'train_loss': 3.721863612596569, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's train!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:32:31.332960Z",
     "start_time": "2023-12-14T02:32:26.595074Z"
    }
   },
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error() # suppress tokenizer warnings\n",
    "\n",
    "prefix = \"Generate Backstory based on following information Character Name: \"\n",
    "\n",
    "prompts = [\n",
    "    \"Frogger Character Race: Aarakocra Character Class: Ranger Output: \",\n",
    "    \"Smarty Character Race: Aasimar Character Class: Cleric Output: \",\n",
    "    \"Volcano Character Race: Android Character Class: Paladin Output: \",\n",
    "]\n",
    "\n",
    "table = wandb.Table(columns=[\"prompt\", \"generation\"])\n",
    "\n",
    "for prompt in prompts:\n",
    "    input_ids = tokenizer.encode(prefix + prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, do_sample=True, max_new_tokens=50, top_p=0.3)\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    table.add_data(prefix + prompt, output_text)\n",
    "    \n",
    "wandb.log({'tiny_generations': table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:32:47.984736Z",
     "start_time": "2023-12-14T02:32:47.977595Z"
    }
   },
   "source": [
    "**Note**: LLM's don't always generate the same results. Your generated characters and backstories may differ from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T02:33:09.434371Z",
     "start_time": "2023-12-14T02:33:02.529300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▄▃▃▄▂▂▂▁▂▄▃▃▃▁▄▁▃▂▂▄▂▂▂▃▃▂▂▂▁▂▃▁▃▁▃▃▂▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>3.35358</td></tr><tr><td>eval/runtime</td><td>130.5342</td></tr><tr><td>eval/samples_per_second</td><td>3.562</td></tr><tr><td>eval/steps_per_second</td><td>0.452</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>233</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>4.5971</td></tr><tr><td>train/total_flos</td><td>40423258718208.0</td></tr><tr><td>train/train_loss</td><td>3.72186</td></tr><tr><td>train/train_runtime</td><td>1783.401</td></tr><tr><td>train/train_samples_per_second</td><td>1.041</td></tr><tr><td>train/train_steps_per_second</td><td>0.131</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devout-lion-2</strong> at: <a href='https://wandb.ai/fabio-bd-araujo/dlai_lm_tuning/runs/3sfjxpr8' target=\"_blank\">https://wandb.ai/fabio-bd-araujo/dlai_lm_tuning/runs/3sfjxpr8</a><br/> View job at <a href='https://wandb.ai/fabio-bd-araujo/dlai_lm_tuning/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMzQ0NTIxOQ==/version_details/v0' target=\"_blank\">https://wandb.ai/fabio-bd-araujo/dlai_lm_tuning/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMzQ0NTIxOQ==/version_details/v0</a><br/>Synced 6 W&B file(s), 1 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231213_230210-3sfjxpr8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
